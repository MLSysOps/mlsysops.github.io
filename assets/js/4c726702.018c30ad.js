"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8909],{3905:(e,a,t)=>{t.d(a,{Zo:()=>m,kt:()=>d});var r=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function p(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var o=r.createContext({}),s=function(e){var a=r.useContext(o),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},m=function(e){var a=s(e.components);return r.createElement(o.Provider,{value:a},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},h=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,i=e.originalType,o=e.parentName,m=p(e,["components","mdxType","originalType","parentName"]),u=s(t),h=n,d=u["".concat(o,".").concat(h)]||u[h]||c[h]||i;return t?r.createElement(d,l(l({ref:a},m),{},{components:t})):r.createElement(d,l({ref:a},m))}));function d(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var i=t.length,l=new Array(i);l[0]=h;var p={};for(var o in a)hasOwnProperty.call(a,o)&&(p[o]=a[o]);p.originalType=e,p[u]="string"==typeof e?e:n,l[1]=p;for(var s=2;s<i;s++)l[s]=t[s];return r.createElement.apply(null,l)}return r.createElement.apply(null,t)}h.displayName="MDXCreateElement"},3292:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>p,toc:()=>s});var r=t(7462),n=(t(7294),t(3905));const i={},l="Training System",p={unversionedId:"paper/General System/training",id:"paper/General System/training",title:"Training System",description:"System for deep learning training. Currently, I only summarize some arxiv papers here and put accepted papers into conference section",source:"@site/docs/paper/General System/training.md",sourceDirName:"paper/General System",slug:"/paper/General System/training",permalink:"/docs/paper/General System/training",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/paper/General System/training.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Machine Learning Infrastructure",permalink:"/docs/paper/General System/infra"},next:{title:"AutoML System",permalink:"/docs/paper/Specific System/AutoML_system"}},o={},s=[{value:"Survey",id:"survey",level:2},{value:"Training(Multi-jobs on cluster)",id:"trainingmulti-jobs-on-cluster",level:2},{value:"Training(Parallelism)",id:"trainingparallelism",level:2}],m={toc:s};function u(e){let{components:a,...t}=e;return(0,n.kt)("wrapper",(0,r.Z)({},m,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"training-system"},"Training System"),(0,n.kt)("p",null,"System for deep learning training. Currently, I only summarize some arxiv papers here and put accepted papers into ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning/tree/master/note"},"conference")," section"),(0,n.kt)("h2",{id:"survey"},"Survey"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},'Mayer, Ruben, and Hans-Arno Jacobsen. "Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques, and Tools." ACM Computing Surveys (CSUR) 53.1 (2020): 1-37. ',(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1903.11314.pdf"},"[Paper]"))),(0,n.kt)("h2",{id:"trainingmulti-jobs-on-cluster"},"Training(Multi-jobs on cluster)"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2008.12260.pdf"},"[arxiv]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/petuum/adaptdl"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"arXiv preprint arXiv:2008.12260 (2020)."),(0,n.kt)("li",{parentName:"ul"},"Qiao, Aurick, Willie Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, and Eric P. Xing"))),(0,n.kt)("li",{parentName:"ul"},"Themis: Fair and Efficient {GPU} Cluster Scheduling. ",(0,n.kt)("a",{parentName:"li",href:"http://wisr.cs.wisc.edu/papers/nsdi20-themis.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Mahajan, K., Balasubramanian, A., Singhvi, A., Venkataraman, S., Akella, A., Phanishayee, A. and Chawla, S., 2020. Themis: Fair and Efficient {GPU} Cluster Scheduling."),(0,n.kt)("li",{parentName:"ul"},"In 17th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 20) (pp. 289-304)."))),(0,n.kt)("li",{parentName:"ul"},"Tiresias: A {GPU} cluster manager for distributed deep learning. ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/nsdi19-gu.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/SymbioticLab/Tiresias"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Gu, J., Chowdhury, M., Shin, K.G., Zhu, Y., Jeon, M., Qian, J., Liu, H. and Guo, C., 2019. "),(0,n.kt)("li",{parentName:"ul"},"In 16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19) (pp. 485-500)."))),(0,n.kt)("li",{parentName:"ul"},"Microsoft OpenPAI HiveDScheduler: As one standalone component of Microsoft OpenPAI, HiveD is designed to be a Kubernetes Scheduler Extender for Multi-Tenant GPU clusters. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/microsoft/hivedscheduler"},"[Project]")),(0,n.kt)("li",{parentName:"ul"},"Gandiva: Introspective cluster scheduling for deep learning. ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/osdi18-xiao.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Xiao, Wencong, et al. (",(0,n.kt)("em",{parentName:"li"},"OSDI 2018"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: Improvet the efficency of hyper-parameter in cluster. Aware of hardware utilization."))),(0,n.kt)("li",{parentName:"ul"},"Optimus: an efficient dynamic resource scheduler for deep learning clusters ",(0,n.kt)("a",{parentName:"li",href:"https://i.cs.hku.hk/~cwu/papers/yhpeng-eurosys18.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Peng, Yanghua, et al. (",(0,n.kt)("em",{parentName:"li"},"EuroSys 2018"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: Job scheduling on clusters. Total complete time as the metric."))),(0,n.kt)("li",{parentName:"ul"},"Multi-tenant GPU clusters for deep learning workloads: Analysis and implications. ",(0,n.kt)("a",{parentName:"li",href:"https://www.microsoft.com/en-us/research/uploads/prod/2018/05/gpu_sched_tr.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/msr-fiddle/philly-traces"},"[dataset]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Jeon, Myeongjae, Shivaram Venkataraman, Junjie Qian, Amar Phanishayee, Wencong Xiao, and Fan Yang"))),(0,n.kt)("li",{parentName:"ul"},"Slurm: A Highly Scalable Workload Manager ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/SchedMD/slurm"},"[GitHub]"))),(0,n.kt)("h2",{id:"trainingparallelism"},"Training(Parallelism)"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"ZeRO: Memory Optimization Towards Training A Trillion Parameter Models. ",(0,n.kt)("em",{parentName:"li"},"Microsoft Work")," ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1910.02054.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/microsoft/DeepSpeed"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Class materials for a distributed systems lecture series ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/aphyr/distsys-class"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"A Unified Architecture for Accelerating Distributed\nDNN Training in Heterogeneous GPU/CPU Clusters ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/osdi20-jiang.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/bytedance/byteps"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, Chuanxiong Guo. (OSDI 2020)"),(0,n.kt)("li",{parentName:"ul"},"Summary: SoTA Parameter Server"))),(0,n.kt)("li",{parentName:"ul"},"PipeDream: Generalized Pipeline Parallelism for DNN Training (SOSP2019) ",(0,n.kt)("a",{parentName:"li",href:"https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/msr-fiddle/pipedream"},"[Github]")),(0,n.kt)("li",{parentName:"ul"},"Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks. ",(0,n.kt)("a",{parentName:"li",href:"http://proceedings.mlr.press/v80/jia18a/jia18a.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/flexflow/FlexFlow"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Zhihao Jia, Sina Lin, Charles R. Qi, and Alex Aiken. (",(0,n.kt)("em",{parentName:"li"},"ICML 2018"),")"))),(0,n.kt)("li",{parentName:"ul"},"Mesh-TensorFlow: Deep Learning for Supercomputers ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1811.02084.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/tensorflow/mesh"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Shazeer, Noam, Youlong Cheng, Niki Parmar, Dustin Tran, et al. (",(0,n.kt)("em",{parentName:"li"},"NIPS 2018"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: Data parallelism for language model"))),(0,n.kt)("li",{parentName:"ul"},"PyTorch-BigGraph: A Large-scale Graph Embedding System ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1903.12287.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/PyTorch-BigGraph"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Lerer, Adam and Wu, Ledell and Shen, Jiajun and Lacroix, Timothee and Wehrstedt, Luca and Bose, Abhijit and Peysakhovich, Alex (",(0,n.kt)("em",{parentName:"li"},"SysML 2019"),")"))),(0,n.kt)("li",{parentName:"ul"},"Beyond data and model parallelism for deep neural networks ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1807.05358.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/jiazhihao/metaflow_sysml19"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Jia, Zhihao, Matei Zaharia, and Alex Aiken. (",(0,n.kt)("em",{parentName:"li"},"SysML 2019"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: SOAP (sample, operation, attribution and parameter) parallelism. Operator graph, device topology and extution optimizer. MCMC search algorithm and excution simulator."))),(0,n.kt)("li",{parentName:"ul"},"Device placement optimization with reinforcement learning ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1706.04972.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Mirhoseini, Azalia, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. (",(0,n.kt)("em",{parentName:"li"},"ICML 17"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: Using REINFORCE learn a device placement policy. Group operations to excute. Need a lot of GPUs."))),(0,n.kt)("li",{parentName:"ul"},"Spotlight: Optimizing device placement for training deep neural networks  ",(0,n.kt)("a",{parentName:"li",href:"http://proceedings.mlr.press/v80/gao18a/gao18a.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Gao, Yuanxiang, Li Chen, and Baochun Li (",(0,n.kt)("em",{parentName:"li"},"ICML 18"),")"))),(0,n.kt)("li",{parentName:"ul"},"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1811.06965.pdf"},"[Paper]"),(0,n.kt)("a",{parentName:"li",href:"https://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.py"},"[GitHub]")," ",(0,n.kt)("a",{parentName:"li",href:"https://www.cnbeta.com/articles/tech/824495.htm"},"[News]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Huang, Yanping, et al. (",(0,n.kt)("em",{parentName:"li"},"arXiv preprint arXiv:1811.06965 (2018)"),")"))),(0,n.kt)("li",{parentName:"ul"},"Horovod: Distributed training framework for TensorFlow, Keras, and PyTorch.\n",(0,n.kt)("a",{parentName:"li",href:"https://github.com/uber/horovod"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Distributed machine learning infrastructure for large-scale robotics research ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/google-research/tensor2robot"},"[GitHub]")," ",(0,n.kt)("a",{parentName:"li",href:"https://ai.google/research/teams/brain/robotics/"},"[Blog]")),(0,n.kt)("li",{parentName:"ul"},"A Generic Communication Scheduler for Distributed DNN Training Acceleration ",(0,n.kt)("a",{parentName:"li",href:"https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/bytedance/byteps"},"[BytePS]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"PENG, Y., Zhu, Y., CHEN, Y., BAO, Y., Yi, B., Lan, C., Wu, C. and Guo, (",(0,n.kt)("em",{parentName:"li"},"SOSP 2019"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: communication schedular")))))}u.isMDXComponent=!0}}]);