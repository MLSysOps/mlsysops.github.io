"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4980],{3905:(e,a,t)=>{t.d(a,{Zo:()=>s,kt:()=>d});var r=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var p=r.createContext({}),m=function(e){var a=r.useContext(p),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},s=function(e){var a=m(e.components);return r.createElement(p.Provider,{value:a},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},h=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,i=e.originalType,p=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),c=m(t),h=n,d=c["".concat(p,".").concat(h)]||c[h]||u[h]||i;return t?r.createElement(d,l(l({ref:a},s),{},{components:t})):r.createElement(d,l({ref:a},s))}));function d(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var i=t.length,l=new Array(i);l[0]=h;var o={};for(var p in a)hasOwnProperty.call(a,p)&&(o[p]=a[p]);o.originalType=e,o[c]="string"==typeof e?e:n,l[1]=o;for(var m=2;m<i;m++)l[m]=t[m];return r.createElement.apply(null,l)}return r.createElement.apply(null,t)}h.displayName="MDXCreateElement"},8731:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>m});var r=t(7462),n=(t(7294),t(3905));const i={},l="Inference System",o={unversionedId:"paper/General System/inference",id:"paper/General System/inference",title:"Inference System",description:"System for machine learning inference.",source:"@site/docs/paper/General System/inference.md",sourceDirName:"paper/General System",slug:"/paper/General System/inference",permalink:"/docs/paper/General System/inference",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/paper/General System/inference.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Data Processing",permalink:"/docs/paper/General System/data_processing"},next:{title:"Machine Learning Infrastructure",permalink:"/docs/paper/General System/infra"}},p={},m=[{value:"Benchmark",id:"benchmark",level:2},{value:"Model Management",id:"model-management",level:2},{value:"Model Serving",id:"model-serving",level:2},{value:"Cache for Inference",id:"cache-for-inference",level:2},{value:"Inference Optimization",id:"inference-optimization",level:2},{value:"Cluster Management for Inference (now only contain multi-tenant)",id:"cluster-management-for-inference-now-only-contain-multi-tenant",level:2},{value:"Machine Learning Compiler",id:"machine-learning-compiler",level:2}],s={toc:m};function c(e){let{components:a,...t}=e;return(0,n.kt)("wrapper",(0,r.Z)({},s,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"inference-system"},"Inference System"),(0,n.kt)("p",null,"System for machine learning inference."),(0,n.kt)("h2",{id:"benchmark"},"Benchmark"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},'Wanling Gao, Fei Tang, Jianfeng Zhan, et al. "AIBench: A Datacenter AI Benchmark Suite, BenchCouncil". ',(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2005.03459.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://www.benchcouncil.org/AIBench/index.html"},"[Website]")),(0,n.kt)("li",{parentName:"ul"},"BaiduBench: Benchmarking Deep Learning operations on different hardware. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/baidu-research/DeepBench#inference-benchmark"},"[Github]")),(0,n.kt)("li",{parentName:"ul"},'Reddi, Vijay Janapa, et al. "Mlperf inference benchmark." arXiv preprint arXiv:1911.02549 (2019). ',(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1911.02549.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/mlperf/inference"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},'Bianco, Simone, et al. "Benchmark analysis of representative deep neural network architectures." IEEE Access 6 (2018): 64270-64277. ',(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1810.00736"},"[Paper]")),(0,n.kt)("li",{parentName:"ul"},'Almeida, Mario, et al. "EmBench: Quantifying Performance Variations of Deep Neural Networks across Modern Commodity Devices." The 3rd International Workshop on Deep Learning for Mobile Systems and Applications. 2019. ',(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1905.07346.pdf"},"[Paper]"))),(0,n.kt)("h2",{id:"model-management"},"Model Management"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Model Card Toolkit. The Model Card Toolkit (MCT) streamlines and automates generation of Model Cards ","[1]",", machine learning documents that provide context and transparency into a model's development and performance. ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1810.03993.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/tensorflow/model-card-toolkit"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"DLHub: Model and data serving for science.  ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1811.11213.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Chard, R., Li, Z., Chard, K., Ward, L., Babuji, Y., Woodard, A., Tuecke, S., Blaiszik, B., Franklin, M. and Foster, I., 2019, May. "),(0,n.kt)("li",{parentName:"ul"},"In 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (pp. 283-292). IEEE. "))),(0,n.kt)("li",{parentName:"ul"},"Publishing and Serving Machine Learning Models with DLHub. ",(0,n.kt)("a",{parentName:"li",href:"https://dl.acm.org/doi/10.1145/3332186.3332246"},"[Paper]")),(0,n.kt)("li",{parentName:"ul"},"TRAINS - Auto-Magical Experiment Manager & Version Control for AI ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/allegroai/trains"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"ModelDB: A system to manage ML models ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/mitdbg/modeldb"},"[GitHub]")," ",(0,n.kt)("a",{parentName:"li",href:"https://mitdbg.github.io/modeldb/papers/hilda_modeldb.pdf"},"[MIT short paper]")),(0,n.kt)("li",{parentName:"ul"},"iterative/dvc: Data & models versioning for ML projects, make them shareable and reproducible ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/iterative/dvc"},"[GitHub]"))),(0,n.kt)("h2",{id:"model-serving"},"Model Serving"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Announcing RedisAI 1.0: AI Serving Engine for Real-Time Applications ",(0,n.kt)("a",{parentName:"li",href:"https://redislabs.com/blog/redisai-ai-serving-engine-for-real-time-applications/"},"[Blog]")),(0,n.kt)("li",{parentName:"ul"},"Cloudburst: Stateful Functions-as-a-Service. ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2001.04592.pdf"},"[","Paper","]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/hydro-project/cloudburst"},"[","GitHub","]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Vikram Sreekanti, Chenggang Wu, Xiayue Charles Lin, Johann Schleier-Smith, Joseph E. Gonzalez, Joseph M. Hellerstein, Alexey Tumanov"),(0,n.kt)("li",{parentName:"ul"},"VLDB 2020"),(0,n.kt)("li",{parentName:"ul"},"A stateful FaaS platform.\n(1) feasibility of general-purpose stateful serverless computing.\n(2) Autoscaling via logical disaggregation of storage and compute, state management via physical\ncolocation of caches with compute services.\n(3) LDPC design pattern"))),(0,n.kt)("li",{parentName:"ul"},"Optimizing Prediction Serving on Low-Latency Serverless Dataflow ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2007.05832.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Sreekanti, Vikram, Harikaran Subbaraj, Chenggang Wu, Joseph E. Gonzalez, and Joseph M. Hellerstein."),(0,n.kt)("li",{parentName:"ul"},"arXiv preprint arXiv:2007.05832 (2020)."))),(0,n.kt)("li",{parentName:"ul"},"Serving DNNs like Clockwork: Performance Predictability from the Bottom Up. ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2006.02464.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Gujarati, A., Karimi, R., Alzayat, S., Kaufmann, A., Vigfusson, Y. and Mace, J., 2020."),(0,n.kt)("li",{parentName:"ul"},"OSDI 2020"))),(0,n.kt)("li",{parentName:"ul"},"Swayam: distributed autoscaling to meet SLAs of machine learning inference services with resource efficiency ",(0,n.kt)("a",{parentName:"li",href:"https://www.microsoft.com/en-us/research/uploads/prod/2018/01/2017.Middleware.Swayam.TailLatencyInAzureML.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Gujarati, Arpan, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Bj\xf6rn B. Brandenburg."),(0,n.kt)("li",{parentName:"ul"},"In Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference, pp. 109-120. 2017."),(0,n.kt)("li",{parentName:"ul"},"Summary: a cloud autoscaler. (1) model-based autoscaling that takes into account SLAs and ML inference workload characteristics, (2) a distributed protocol that uses partial load information and prediction at frontends to provi- sion new service instances, and (3) a backend self-decommissioning protocol for service instances"))),(0,n.kt)("li",{parentName:"ul"},"Swift machine learning model serving scheduling: a region based reinforcement learning approach. ",(0,n.kt)("a",{parentName:"li",href:"https://dl.acm.org/doi/10.1145/3295500.3356164"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/SC-RRL/RRL"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Qin, Heyang, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao, and Feng Yan."),(0,n.kt)("li",{parentName:"ul"},"In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-23. 2019."),(0,n.kt)("li",{parentName:"ul"},"Summary: The system performances under different similar con- figurations in a region can be accurately estimated by using the system performance under one of these configurations, due to their similarity. Region based DRL is designed for parallelism selection."))),(0,n.kt)("li",{parentName:"ul"},"TorchServe is a flexible and easy to use tool for serving PyTorch models. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/pytorch/serve"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Seldon Core: Blazing Fast, Industry-Ready ML. An open source platform to deploy your machine learning models on Kubernetes at massive scale. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/SeldonIO/seldon-core"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/atc19-zhang-chengliang.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/marcoszh/MArk-Project"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Zhang, C., Yu, M., Wang, W. and Yan, F., 2019. "),(0,n.kt)("li",{parentName:"ul"},"In 2019 {USENIX} Annual Technical Conference ({USENIX}{ATC} 19) (pp. 1049-1062)."),(0,n.kt)("li",{parentName:"ul"},"Summary: address the scalability and cost minimization issues for model serving on the public cloud."))),(0,n.kt)("li",{parentName:"ul"},"Parity Models: Erasure-Coded Resilience for Prediction Serving Systems(SOSP2019) ",(0,n.kt)("a",{parentName:"li",href:"http://www.cs.cmu.edu/~rvinayak/papers/sosp2019parity-models.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/Thesys-lab/parity-models"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"INFaaS: A Model-less Inference Serving System Romero ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1905.13348"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/stanford-mast/INFaaS"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"F., Li, Q., Yadwadkar, N.J. and Kozyrakis, C., 2019."),(0,n.kt)("li",{parentName:"ul"},"arXiv preprint arXiv:1905.13348."))),(0,n.kt)("li",{parentName:"ul"},"Nexus: Nexus is a scalable and efficient serving system for DNN applications on GPU cluster (SOSP2019) ",(0,n.kt)("a",{parentName:"li",href:"https://pdfs.semanticscholar.org/0c0f/353dbac84311ea4f1485d4a8ac0b0459be8c.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/uwsampl/nexus"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Deep Learning Inference Service at Microsoft ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/opml19papers-soifer.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"J Soifer, et al. (",(0,n.kt)("em",{parentName:"li"},"OptML2019"),")"))),(0,n.kt)("li",{parentName:"ul"},"{PRETZEL}: Opening the Black Box of Machine Learning Prediction Serving Systems. ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/osdi18-lee.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Lee, Y., Scolari, A., Chun, B.G., Santambrogio, M.D., Weimer, M. and Interlandi, M., 2018. (",(0,n.kt)("em",{parentName:"li"},"OSDI 2018"),")"))),(0,n.kt)("li",{parentName:"ul"},"Brusta: PyTorch model serving project ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/hyoungseok/brusta"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Model Server for Apache MXNet: Model Server for Apache MXNet is a tool for serving neural net models for inference ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/awslabs/mxnet-model-server"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"TFX: A TensorFlow-Based Production-Scale Machine Learning Platform ",(0,n.kt)("a",{parentName:"li",href:"http://stevenwhang.com/tfx_paper.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://www.tensorflow.org/tfx"},"[Website]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/tensorflow/tfx"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Baylor, Denis, et al. (",(0,n.kt)("em",{parentName:"li"},"KDD 2017"),")"))),(0,n.kt)("li",{parentName:"ul"},"Tensorflow-serving: Flexible, high-performance ml serving ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1712.06139"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/tensorflow/serving"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Olston, Christopher, et al."))),(0,n.kt)("li",{parentName:"ul"},"IntelAI/OpenVINO-model-server: Inference model server implementation with gRPC interface, compatible with TensorFlow serving API and OpenVINO\u2122 as the execution backend. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/IntelAI/OpenVINO-model-server"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Clipper: A Low-Latency Online Prediction Serving System ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf"},"[Paper]"),(0,n.kt)("a",{parentName:"li",href:"https://github.com/ucbrise/clipper"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Crankshaw, Daniel, et al. (",(0,n.kt)("em",{parentName:"li"},"NSDI 2017"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: Adaptive batch "))),(0,n.kt)("li",{parentName:"ul"},"InferLine: ML Inference Pipeline Composition Framework ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1812.01776.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/simon-mo/inferline-models"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Crankshaw, Daniel, et al. (",(0,n.kt)("em",{parentName:"li"},"Preprint"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: update version of Clipper"))),(0,n.kt)("li",{parentName:"ul"},"TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1811.09732.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Dakkak, Abdul, et al (",(0,n.kt)("em",{parentName:"li"},"Preprint"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: model cold start problem"))),(0,n.kt)("li",{parentName:"ul"},"Rafiki: machine learning as an analytics service system ",(0,n.kt)("a",{parentName:"li",href:"http://www.vldb.org/pvldb/vol12/p128-wang.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/nginyc/rafiki"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Wang, Wei, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad."),(0,n.kt)("li",{parentName:"ul"},"Summary: Contain both training and inference. Auto-Hype-Parameter search for training. Ensemble models for inference. Using DRL to balance trade-off between accuracy and latency."))),(0,n.kt)("li",{parentName:"ul"},"GraphPipe: Machine Learning Model Deployment Made Simple ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/oracle/graphpipe"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Orkhon: ML Inference Framework and Server Runtime ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/vertexclique/orkhon"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"NVIDIA/tensorrt-inference-server: The TensorRT Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVIDIA/tensorrt-inference-server"},"[GitHub]")," ",(0,n.kt)("a",{parentName:"li",href:"https://on-demand.gputechconf.com/gtc-cn/2019/pdf/CN9506/presentation.pdf"},"[Slides: DEEP INTO TRTIS]")," "),(0,n.kt)("li",{parentName:"ul"},"Apache PredictionIO\xae is an open source Machine Learning Server built on top of a state-of-the-art open source stack for developers and data scientists to create predictive engines for any machine learning task ",(0,n.kt)("a",{parentName:"li",href:"http://predictionio.apache.org/"},"[Website]"))),(0,n.kt)("h2",{id:"cache-for-inference"},"Cache for Inference"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},'Kumar, Adarsh, et al. "Accelerating deep learning inference via freezing." 11th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 19). 2019. ',(0,n.kt)("a",{parentName:"li",href:"http://shivaram.org/publications/freeze-hotcloud19.pdf"},"[Paper]")),(0,n.kt)("li",{parentName:"ul"},'Xu, Mengwei, et al. "DeepCache: Principled cache for mobile deep vision." Proceedings of the 24th Annual International Conference on Mobile Computing and Networking. 2018. ',(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1712.01670.pdf"},"[Paper]")),(0,n.kt)("li",{parentName:"ul"},'Park, Keunyoung, and Doo-Hyun Kim. "Accelerating image classification using feature map similarity in convolutional neural networks." Applied Sciences 9.1 (2019): 108. ',(0,n.kt)("a",{parentName:"li",href:"https://www.mdpi.com/2076-3417/9/1/108/htm"},"[Paper]")),(0,n.kt)("li",{parentName:"ul"},'Cavigelli, Lukas, and Luca Benini. "CBinfer: Exploiting frame-to-frame locality for faster convolutional network inference on video streams." IEEE Transactions on Circuits and Systems for Video Technology (2019). ',(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1808.05488"},"[Paper]"))),(0,n.kt)("h2",{id:"inference-optimization"},"Inference Optimization"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Jointly Optimizing Preprocessing and Inference for DNN-based Visual Analytics ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2007.13005.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Daniel Kang, Ankit Mathur, Teja Veeramacheneni, Peter Bailis, Matei Zaharia"),(0,n.kt)("li",{parentName:"ul"},"VLDB 2021 "))),(0,n.kt)("li",{parentName:"ul"},"Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning Inference. ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1906.01974.pdf"},"[arxiv]"),(0,n.kt)("a",{parentName:"li",href:"https://github.com/stanford-futuredata/Willump"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Peter Kraft, Daniel Kang, Deepak Narayanan, Shoumik Palkar, Peter Bailis, Matei Zaharia."),(0,n.kt)("li",{parentName:"ul"},"arXiv Preprint. 2019."))),(0,n.kt)("li",{parentName:"ul"},"TensorRT is a C++ library that facilitates high performance inference on NVIDIA GPUs and deep learning accelerators. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVIDIA/TensorRT"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"Dynamic Space-Time Scheduling for GPU Inference ",(0,n.kt)("a",{parentName:"li",href:"http://learningsys.org/nips18/assets/papers/102CameraReadySubmissionGPU_Virtualization%20(8).pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/ucbrise/caravel"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Jain, Paras, et al. (",(0,n.kt)("em",{parentName:"li"},"NIPS 18, System for ML"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: optimization for GPU Multi-tenancy"))),(0,n.kt)("li",{parentName:"ul"},"Dynamic Scheduling For Dynamic Control Flow in Deep Learning Systems ",(0,n.kt)("a",{parentName:"li",href:"http://www.cs.cmu.edu/~jinlianw/papers/dynamic_scheduling_nips18_sysml.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Wei, Jinliang, Garth Gibson, Vijay Vasudevan, and Eric Xing. (",(0,n.kt)("em",{parentName:"li"},"On going"),")"))),(0,n.kt)("li",{parentName:"ul"},"Accelerating Deep Learning Workloads through Efficient Multi-Model Execution. ",(0,n.kt)("a",{parentName:"li",href:"https://cs.stanford.edu/~matei/papers/2018/mlsys_hivemind.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"D. Narayanan, K. Santhanam, A. Phanishayee and M. Zaharia. (",(0,n.kt)("em",{parentName:"li"},"NeurIPS Systems for ML Workshop 2018"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: They assume that their system, HiveMind, is given as input models grouped into model batches that are amenable to co-optimization and co-execution. a compiler, and a runtime."))),(0,n.kt)("li",{parentName:"ul"},"DeepCPU: Serving RNN-based Deep Learning Models 10x Faster ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/conference/atc18/atc18-zhang-minjia.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Minjia Zhang, Samyam Rajbhandari, Wenhan Wang, and Yuxiong He, Microsoft AI and Research (",(0,n.kt)("em",{parentName:"li"},"ATC 2018"),")")))),(0,n.kt)("h2",{id:"cluster-management-for-inference-now-only-contain-multi-tenant"},"Cluster Management for Inference (now only contain multi-tenant)"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Ease. ml: Towards multi-tenant resource sharing for machine learning workloads ",(0,n.kt)("a",{parentName:"li",href:"http://www.vldb.org/pvldb/vol11/p607-li.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/DS3Lab/easeml"},"[GitHub]")," ",(0,n.kt)("a",{parentName:"li",href:"http://www.vldb.org/pvldb/vol11/p2054-karlas.pdf"},"[Demo]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Li, Tian, et al"),(0,n.kt)("li",{parentName:"ul"},"Proceedings of the VLDB Endowment 11.5 (2018): 607-620."))),(0,n.kt)("li",{parentName:"ul"},"Perseus: Characterizing Performance and Cost of Multi-Tenant Serving for CNN Models ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1912.02322.pdf"},"[Paper]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"LeMay, Matthew, Shijian Li, and Tian Guo. "),(0,n.kt)("li",{parentName:"ul"},"arXiv preprint arXiv:1912.02322 (2019).")))),(0,n.kt)("h2",{id:"machine-learning-compiler"},"Machine Learning Compiler"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Hummingbird: Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models.",(0,n.kt)("a",{parentName:"li",href:"https://github.com/microsoft/hummingbird"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"{TVM}: An Automated End-to-End Optimizing Compiler for Deep Learning ",(0,n.kt)("a",{parentName:"li",href:"https://www.usenix.org/system/files/osdi18-chen.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=I1APhlSjVjs"},"[YouTube]")," ",(0,n.kt)("a",{parentName:"li",href:"https://tvm.ai/"},"[Project Website]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Chen, Tianqi, et al. (",(0,n.kt)("em",{parentName:"li"},"OSDI 2018"),")"),(0,n.kt)("li",{parentName:"ul"},"Summary: Automated optimization is very impressive: cost model (rank objective function) + schedule explorer (parallel simulated annealing)"))),(0,n.kt)("li",{parentName:"ul"},"Facebook TC: Tensor Comprehensions (TC) is a fully-functional C++ library to automatically synthesize high-performance machine learning kernels using Halide, ISL and NVRTC or LLVM. ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/TensorComprehensions"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},'Tensorflow/mlir: "Multi-Level Intermediate Representation" Compiler Infrastructure ',(0,n.kt)("a",{parentName:"li",href:"https://github.com/tensorflow/mlir"},"[GitHub]")," ",(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=qzljG6DKgic"},"[Video]")),(0,n.kt)("li",{parentName:"ul"},"PyTorch/glow: Compiler for Neural Network hardware accelerators ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/pytorch/glow"},"[GitHub]")),(0,n.kt)("li",{parentName:"ul"},"TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions ",(0,n.kt)("a",{parentName:"li",href:"https://cs.stanford.edu/~matei/papers/2019/sosp_taso.pdf"},"[Paper]")," ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/jiazhihao/TASO"},"[GitHub]"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Jia, Zhihao, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken. (",(0,n.kt)("em",{parentName:"li"},"SOSP 2019"),")"),(0,n.kt)("li",{parentName:"ul"},"Experiments tested on TVM and XLA")))))}c.isMDXComponent=!0}}]);