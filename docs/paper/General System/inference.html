<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-paper/General System/inference">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Inference System | Awesome AI Engineering</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mlsysops.github.io/docs/paper/General System/inference"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Inference System | Awesome AI Engineering"><meta data-rh="true" name="description" content="System for machine learning inference."><meta data-rh="true" property="og:description" content="System for machine learning inference."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://mlsysops.github.io/docs/paper/General System/inference"><link data-rh="true" rel="alternate" href="https://mlsysops.github.io/docs/paper/General System/inference" hreflang="en"><link data-rh="true" rel="alternate" href="https://mlsysops.github.io/docs/paper/General System/inference" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Awesome AI Engineering RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Awesome AI Engineering Atom Feed"><link rel="stylesheet" href="/assets/css/styles.f860236f.css">
<link rel="preload" href="/assets/js/runtime~main.54756531.js" as="script">
<link rel="preload" href="/assets/js/main.ec3d4655.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Awesome AI Engineering</b></a><a class="navbar__item navbar__link" href="/docs/course">Course</a><a class="navbar__item navbar__link" href="/docs/blog">Blog</a><a class="navbar__item navbar__link" href="/docs/talk">Talk</a><a class="navbar__item navbar__link" href="/docs/group">Group</a><a class="navbar__item navbar__link" href="/docs/paper/intro">Paper</a><a class="navbar__item navbar__link" href="/docs/tutorial/MLOps_101">Tutorial</a></div><div class="navbar__items navbar__items--right"><a href="https://breezeml.ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">BreezeML<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/quick_start">AI Engineering Quick Start</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/course">Course</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/blog">Blog</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/talk">Talk</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/group">Research Group</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs/category/papers">Papers</a><button aria-label="Toggle the collapsible sidebar category &#x27;Papers&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/paper/General System/data_processing">General System</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/paper/General System/data_processing">Data Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/paper/General System/inference">Inference System</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/paper/General System/infra">Machine Learning Infrastructure</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/paper/General System/training">Training System</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/paper/Specific System/AutoML_system">Specific System</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/paper/intro">Survey &amp; Book</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial">Tutorial</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/papers"><span itemprop="name">Papers</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">General System</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Inference System</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Inference System</h1><p>System for machine learning inference.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark">Benchmark<a class="hash-link" href="#benchmark" title="Direct link to heading">​</a></h2><ul><li>Wanling Gao, Fei Tang, Jianfeng Zhan, et al. &quot;AIBench: A Datacenter AI Benchmark Suite, BenchCouncil&quot;. <a href="https://arxiv.org/pdf/2005.03459.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://www.benchcouncil.org/AIBench/index.html" target="_blank" rel="noopener noreferrer">[Website]</a></li><li>BaiduBench: Benchmarking Deep Learning operations on different hardware. <a href="https://github.com/baidu-research/DeepBench#inference-benchmark" target="_blank" rel="noopener noreferrer">[Github]</a></li><li>Reddi, Vijay Janapa, et al. &quot;Mlperf inference benchmark.&quot; arXiv preprint arXiv:1911.02549 (2019). <a href="https://arxiv.org/pdf/1911.02549.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/mlperf/inference" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Bianco, Simone, et al. &quot;Benchmark analysis of representative deep neural network architectures.&quot; IEEE Access 6 (2018): 64270-64277. <a href="https://arxiv.org/abs/1810.00736" target="_blank" rel="noopener noreferrer">[Paper]</a></li><li>Almeida, Mario, et al. &quot;EmBench: Quantifying Performance Variations of Deep Neural Networks across Modern Commodity Devices.&quot; The 3rd International Workshop on Deep Learning for Mobile Systems and Applications. 2019. <a href="https://arxiv.org/pdf/1905.07346.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-management">Model Management<a class="hash-link" href="#model-management" title="Direct link to heading">​</a></h2><ul><li>Model Card Toolkit. The Model Card Toolkit (MCT) streamlines and automates generation of Model Cards <!-- -->[1]<!-- -->, machine learning documents that provide context and transparency into a model&#x27;s development and performance. <a href="https://arxiv.org/pdf/1810.03993.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/tensorflow/model-card-toolkit" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>DLHub: Model and data serving for science.  <a href="https://arxiv.org/pdf/1811.11213.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Chard, R., Li, Z., Chard, K., Ward, L., Babuji, Y., Woodard, A., Tuecke, S., Blaiszik, B., Franklin, M. and Foster, I., 2019, May. </li><li>In 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (pp. 283-292). IEEE. </li></ul></li><li>Publishing and Serving Machine Learning Models with DLHub. <a href="https://dl.acm.org/doi/10.1145/3332186.3332246" target="_blank" rel="noopener noreferrer">[Paper]</a></li><li>TRAINS - Auto-Magical Experiment Manager &amp; Version Control for AI <a href="https://github.com/allegroai/trains" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>ModelDB: A system to manage ML models <a href="https://github.com/mitdbg/modeldb" target="_blank" rel="noopener noreferrer">[GitHub]</a> <a href="https://mitdbg.github.io/modeldb/papers/hilda_modeldb.pdf" target="_blank" rel="noopener noreferrer">[MIT short paper]</a></li><li>iterative/dvc: Data &amp; models versioning for ML projects, make them shareable and reproducible <a href="https://github.com/iterative/dvc" target="_blank" rel="noopener noreferrer">[GitHub]</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-serving">Model Serving<a class="hash-link" href="#model-serving" title="Direct link to heading">​</a></h2><ul><li>Announcing RedisAI 1.0: AI Serving Engine for Real-Time Applications <a href="https://redislabs.com/blog/redisai-ai-serving-engine-for-real-time-applications/" target="_blank" rel="noopener noreferrer">[Blog]</a></li><li>Cloudburst: Stateful Functions-as-a-Service. <a href="https://arxiv.org/pdf/2001.04592.pdf" target="_blank" rel="noopener noreferrer">[<!-- -->Paper<!-- -->]</a> <a href="https://github.com/hydro-project/cloudburst" target="_blank" rel="noopener noreferrer">[<!-- -->GitHub<!-- -->]</a><ul><li>Vikram Sreekanti, Chenggang Wu, Xiayue Charles Lin, Johann Schleier-Smith, Joseph E. Gonzalez, Joseph M. Hellerstein, Alexey Tumanov</li><li>VLDB 2020</li><li>A stateful FaaS platform.
(1) feasibility of general-purpose stateful serverless computing.
(2) Autoscaling via logical disaggregation of storage and compute, state management via physical
colocation of caches with compute services.
(3) LDPC design pattern</li></ul></li><li>Optimizing Prediction Serving on Low-Latency Serverless Dataflow <a href="https://arxiv.org/pdf/2007.05832.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Sreekanti, Vikram, Harikaran Subbaraj, Chenggang Wu, Joseph E. Gonzalez, and Joseph M. Hellerstein.</li><li>arXiv preprint arXiv:2007.05832 (2020).</li></ul></li><li>Serving DNNs like Clockwork: Performance Predictability from the Bottom Up. <a href="https://arxiv.org/pdf/2006.02464.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Gujarati, A., Karimi, R., Alzayat, S., Kaufmann, A., Vigfusson, Y. and Mace, J., 2020.</li><li>OSDI 2020</li></ul></li><li>Swayam: distributed autoscaling to meet SLAs of machine learning inference services with resource efficiency <a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/01/2017.Middleware.Swayam.TailLatencyInAzureML.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Gujarati, Arpan, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg.</li><li>In Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference, pp. 109-120. 2017.</li><li>Summary: a cloud autoscaler. (1) model-based autoscaling that takes into account SLAs and ML inference workload characteristics, (2) a distributed protocol that uses partial load information and prediction at frontends to provi- sion new service instances, and (3) a backend self-decommissioning protocol for service instances</li></ul></li><li>Swift machine learning model serving scheduling: a region based reinforcement learning approach. <a href="https://dl.acm.org/doi/10.1145/3295500.3356164" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/SC-RRL/RRL" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Qin, Heyang, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao, and Feng Yan.</li><li>In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-23. 2019.</li><li>Summary: The system performances under different similar con- figurations in a region can be accurately estimated by using the system performance under one of these configurations, due to their similarity. Region based DRL is designed for parallelism selection.</li></ul></li><li>TorchServe is a flexible and easy to use tool for serving PyTorch models. <a href="https://github.com/pytorch/serve" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Seldon Core: Blazing Fast, Industry-Ready ML. An open source platform to deploy your machine learning models on Kubernetes at massive scale. <a href="https://github.com/SeldonIO/seldon-core" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving <a href="https://www.usenix.org/system/files/atc19-zhang-chengliang.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/marcoszh/MArk-Project" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Zhang, C., Yu, M., Wang, W. and Yan, F., 2019. </li><li>In 2019 {USENIX} Annual Technical Conference ({USENIX}{ATC} 19) (pp. 1049-1062).</li><li>Summary: address the scalability and cost minimization issues for model serving on the public cloud.</li></ul></li><li>Parity Models: Erasure-Coded Resilience for Prediction Serving Systems(SOSP2019) <a href="http://www.cs.cmu.edu/~rvinayak/papers/sosp2019parity-models.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/Thesys-lab/parity-models" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>INFaaS: A Model-less Inference Serving System Romero <a href="https://arxiv.org/abs/1905.13348" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/stanford-mast/INFaaS" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>F., Li, Q., Yadwadkar, N.J. and Kozyrakis, C., 2019.</li><li>arXiv preprint arXiv:1905.13348.</li></ul></li><li>Nexus: Nexus is a scalable and efficient serving system for DNN applications on GPU cluster (SOSP2019) <a href="https://pdfs.semanticscholar.org/0c0f/353dbac84311ea4f1485d4a8ac0b0459be8c.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/uwsampl/nexus" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Deep Learning Inference Service at Microsoft <a href="https://www.usenix.org/system/files/opml19papers-soifer.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>J Soifer, et al. (<em>OptML2019</em>)</li></ul></li><li>{PRETZEL}: Opening the Black Box of Machine Learning Prediction Serving Systems. <a href="https://www.usenix.org/system/files/osdi18-lee.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Lee, Y., Scolari, A., Chun, B.G., Santambrogio, M.D., Weimer, M. and Interlandi, M., 2018. (<em>OSDI 2018</em>)</li></ul></li><li>Brusta: PyTorch model serving project <a href="https://github.com/hyoungseok/brusta" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Model Server for Apache MXNet: Model Server for Apache MXNet is a tool for serving neural net models for inference <a href="https://github.com/awslabs/mxnet-model-server" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>TFX: A TensorFlow-Based Production-Scale Machine Learning Platform <a href="http://stevenwhang.com/tfx_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://www.tensorflow.org/tfx" target="_blank" rel="noopener noreferrer">[Website]</a> <a href="https://github.com/tensorflow/tfx" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Baylor, Denis, et al. (<em>KDD 2017</em>)</li></ul></li><li>Tensorflow-serving: Flexible, high-performance ml serving <a href="https://arxiv.org/pdf/1712.06139" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/tensorflow/serving" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Olston, Christopher, et al.</li></ul></li><li>IntelAI/OpenVINO-model-server: Inference model server implementation with gRPC interface, compatible with TensorFlow serving API and OpenVINO™ as the execution backend. <a href="https://github.com/IntelAI/OpenVINO-model-server" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Clipper: A Low-Latency Online Prediction Serving System <a href="https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><a href="https://github.com/ucbrise/clipper" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Crankshaw, Daniel, et al. (<em>NSDI 2017</em>)</li><li>Summary: Adaptive batch </li></ul></li><li>InferLine: ML Inference Pipeline Composition Framework <a href="https://arxiv.org/pdf/1812.01776.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/simon-mo/inferline-models" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Crankshaw, Daniel, et al. (<em>Preprint</em>)</li><li>Summary: update version of Clipper</li></ul></li><li>TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments <a href="https://arxiv.org/pdf/1811.09732.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Dakkak, Abdul, et al (<em>Preprint</em>)</li><li>Summary: model cold start problem</li></ul></li><li>Rafiki: machine learning as an analytics service system <a href="http://www.vldb.org/pvldb/vol12/p128-wang.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/nginyc/rafiki" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Wang, Wei, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad.</li><li>Summary: Contain both training and inference. Auto-Hype-Parameter search for training. Ensemble models for inference. Using DRL to balance trade-off between accuracy and latency.</li></ul></li><li>GraphPipe: Machine Learning Model Deployment Made Simple <a href="https://github.com/oracle/graphpipe" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Orkhon: ML Inference Framework and Server Runtime <a href="https://github.com/vertexclique/orkhon" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>NVIDIA/tensorrt-inference-server: The TensorRT Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs. <a href="https://github.com/NVIDIA/tensorrt-inference-server" target="_blank" rel="noopener noreferrer">[GitHub]</a> <a href="https://on-demand.gputechconf.com/gtc-cn/2019/pdf/CN9506/presentation.pdf" target="_blank" rel="noopener noreferrer">[Slides: DEEP INTO TRTIS]</a> </li><li>Apache PredictionIO® is an open source Machine Learning Server built on top of a state-of-the-art open source stack for developers and data scientists to create predictive engines for any machine learning task <a href="http://predictionio.apache.org/" target="_blank" rel="noopener noreferrer">[Website]</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cache-for-inference">Cache for Inference<a class="hash-link" href="#cache-for-inference" title="Direct link to heading">​</a></h2><ul><li>Kumar, Adarsh, et al. &quot;Accelerating deep learning inference via freezing.&quot; 11th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 19). 2019. <a href="http://shivaram.org/publications/freeze-hotcloud19.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></li><li>Xu, Mengwei, et al. &quot;DeepCache: Principled cache for mobile deep vision.&quot; Proceedings of the 24th Annual International Conference on Mobile Computing and Networking. 2018. <a href="https://arxiv.org/pdf/1712.01670.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></li><li>Park, Keunyoung, and Doo-Hyun Kim. &quot;Accelerating image classification using feature map similarity in convolutional neural networks.&quot; Applied Sciences 9.1 (2019): 108. <a href="https://www.mdpi.com/2076-3417/9/1/108/htm" target="_blank" rel="noopener noreferrer">[Paper]</a></li><li>Cavigelli, Lukas, and Luca Benini. &quot;CBinfer: Exploiting frame-to-frame locality for faster convolutional network inference on video streams.&quot; IEEE Transactions on Circuits and Systems for Video Technology (2019). <a href="https://arxiv.org/pdf/1808.05488" target="_blank" rel="noopener noreferrer">[Paper]</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="inference-optimization">Inference Optimization<a class="hash-link" href="#inference-optimization" title="Direct link to heading">​</a></h2><ul><li>Jointly Optimizing Preprocessing and Inference for DNN-based Visual Analytics <a href="https://arxiv.org/pdf/2007.13005.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Daniel Kang, Ankit Mathur, Teja Veeramacheneni, Peter Bailis, Matei Zaharia</li><li>VLDB 2021 </li></ul></li><li>Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning Inference. <a href="https://arxiv.org/pdf/1906.01974.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a><a href="https://github.com/stanford-futuredata/Willump" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Peter Kraft, Daniel Kang, Deepak Narayanan, Shoumik Palkar, Peter Bailis, Matei Zaharia.</li><li>arXiv Preprint. 2019.</li></ul></li><li>TensorRT is a C++ library that facilitates high performance inference on NVIDIA GPUs and deep learning accelerators. <a href="https://github.com/NVIDIA/TensorRT" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Dynamic Space-Time Scheduling for GPU Inference <a href="http://learningsys.org/nips18/assets/papers/102CameraReadySubmissionGPU_Virtualization%20(8).pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/ucbrise/caravel" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Jain, Paras, et al. (<em>NIPS 18, System for ML</em>)</li><li>Summary: optimization for GPU Multi-tenancy</li></ul></li><li>Dynamic Scheduling For Dynamic Control Flow in Deep Learning Systems <a href="http://www.cs.cmu.edu/~jinlianw/papers/dynamic_scheduling_nips18_sysml.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Wei, Jinliang, Garth Gibson, Vijay Vasudevan, and Eric Xing. (<em>On going</em>)</li></ul></li><li>Accelerating Deep Learning Workloads through Efficient Multi-Model Execution. <a href="https://cs.stanford.edu/~matei/papers/2018/mlsys_hivemind.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>D. Narayanan, K. Santhanam, A. Phanishayee and M. Zaharia. (<em>NeurIPS Systems for ML Workshop 2018</em>)</li><li>Summary: They assume that their system, HiveMind, is given as input models grouped into model batches that are amenable to co-optimization and co-execution. a compiler, and a runtime.</li></ul></li><li>DeepCPU: Serving RNN-based Deep Learning Models 10x Faster <a href="https://www.usenix.org/system/files/conference/atc18/atc18-zhang-minjia.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>Minjia Zhang, Samyam Rajbhandari, Wenhan Wang, and Yuxiong He, Microsoft AI and Research (<em>ATC 2018</em>)</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cluster-management-for-inference-now-only-contain-multi-tenant">Cluster Management for Inference (now only contain multi-tenant)<a class="hash-link" href="#cluster-management-for-inference-now-only-contain-multi-tenant" title="Direct link to heading">​</a></h2><ul><li>Ease. ml: Towards multi-tenant resource sharing for machine learning workloads <a href="http://www.vldb.org/pvldb/vol11/p607-li.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/DS3Lab/easeml" target="_blank" rel="noopener noreferrer">[GitHub]</a> <a href="http://www.vldb.org/pvldb/vol11/p2054-karlas.pdf" target="_blank" rel="noopener noreferrer">[Demo]</a><ul><li>Li, Tian, et al</li><li>Proceedings of the VLDB Endowment 11.5 (2018): 607-620.</li></ul></li><li>Perseus: Characterizing Performance and Cost of Multi-Tenant Serving for CNN Models <a href="https://arxiv.org/pdf/1912.02322.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><ul><li>LeMay, Matthew, Shijian Li, and Tian Guo. </li><li>arXiv preprint arXiv:1912.02322 (2019).</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="machine-learning-compiler">Machine Learning Compiler<a class="hash-link" href="#machine-learning-compiler" title="Direct link to heading">​</a></h2><ul><li>Hummingbird: Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models.<a href="https://github.com/microsoft/hummingbird" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>{TVM}: An Automated End-to-End Optimizing Compiler for Deep Learning <a href="https://www.usenix.org/system/files/osdi18-chen.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://www.youtube.com/watch?v=I1APhlSjVjs" target="_blank" rel="noopener noreferrer">[YouTube]</a> <a href="https://tvm.ai/" target="_blank" rel="noopener noreferrer">[Project Website]</a><ul><li>Chen, Tianqi, et al. (<em>OSDI 2018</em>)</li><li>Summary: Automated optimization is very impressive: cost model (rank objective function) + schedule explorer (parallel simulated annealing)</li></ul></li><li>Facebook TC: Tensor Comprehensions (TC) is a fully-functional C++ library to automatically synthesize high-performance machine learning kernels using Halide, ISL and NVRTC or LLVM. <a href="https://github.com/facebookresearch/TensorComprehensions" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>Tensorflow/mlir: &quot;Multi-Level Intermediate Representation&quot; Compiler Infrastructure <a href="https://github.com/tensorflow/mlir" target="_blank" rel="noopener noreferrer">[GitHub]</a> <a href="https://www.youtube.com/watch?v=qzljG6DKgic" target="_blank" rel="noopener noreferrer">[Video]</a></li><li>PyTorch/glow: Compiler for Neural Network hardware accelerators <a href="https://github.com/pytorch/glow" target="_blank" rel="noopener noreferrer">[GitHub]</a></li><li>TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions <a href="https://cs.stanford.edu/~matei/papers/2019/sosp_taso.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/jiazhihao/TASO" target="_blank" rel="noopener noreferrer">[GitHub]</a><ul><li>Jia, Zhihao, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken. (<em>SOSP 2019</em>)</li><li>Experiments tested on TVM and XLA</li></ul></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/paper/General System/inference.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/paper/General System/data_processing"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Data Processing</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/paper/General System/infra"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Machine Learning Infrastructure</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#benchmark" class="table-of-contents__link toc-highlight">Benchmark</a></li><li><a href="#model-management" class="table-of-contents__link toc-highlight">Model Management</a></li><li><a href="#model-serving" class="table-of-contents__link toc-highlight">Model Serving</a></li><li><a href="#cache-for-inference" class="table-of-contents__link toc-highlight">Cache for Inference</a></li><li><a href="#inference-optimization" class="table-of-contents__link toc-highlight">Inference Optimization</a></li><li><a href="#cluster-management-for-inference-now-only-contain-multi-tenant" class="table-of-contents__link toc-highlight">Cluster Management for Inference (now only contain multi-tenant)</a></li><li><a href="#machine-learning-compiler" class="table-of-contents__link toc-highlight">Machine Learning Compiler</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/quick_start">Quick Start</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://breezeml.ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">BreezeML<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Visitor From:</div><ul class="footer__items clean-list"><li class="footer__item">
<a href="https://info.flagcounter.com/5njl"><img src="https://s11.flagcounter.com/count2/5njl/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 MLSysOps, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.54756531.js"></script>
<script src="/assets/js/main.ec3d4655.js"></script>
</body>
</html>